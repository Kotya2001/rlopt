{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563ca54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ortools.linear_solver import pywraplp\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0299bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 7 # num vars\n",
    "n = 3  # num constraints\n",
    "ubound = 1 # upper bound of constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439e7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data simpling\n",
    "rand = np.random.RandomState(3)\n",
    "p = np.round(rand.random_sample(m)*5,1) # goal koef\n",
    "c = np.round(rand.random_sample((n,m))*10 * (rand.random_sample(m)*(p/5)*0.3 + 1),1) # constrants\n",
    "b = np.round(c.sum(axis = 1) * (rand.random_sample(n) * 0.5 + 0.3),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9e9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = pywraplp.Solver.CreateSolver('SCIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0331ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "for j in range(m):\n",
    "    x[j] = solver.IntVar(0, ubound, f\"x[{j}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450f4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    constraint_expr = [c[i,j] * x[j] for j in range(m)]\n",
    "    solver.Add(sum(constraint_expr) <= b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1555d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_expr = [p[j] * x[j] for j in range(m)]\n",
    "solver.Maximize(solver.Sum(obj_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac54271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver.EnableOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080bdc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = solver.Solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83ed1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value = 10.600000000000001\n",
      "x[0]  =  0.0\n",
      "x[1]  =  1.0\n",
      "x[2]  =  0.0\n",
      "x[3]  =  1.0\n",
      "x[4]  =  1.0\n",
      "x[5]  =  0.0\n",
      "x[6]  =  0.0\n",
      "\n",
      "Problem solved in 119 milliseconds\n",
      "Problem solved in 7 iterations\n",
      "Problem solved in 1 branch-and-bound nodes\n"
     ]
    }
   ],
   "source": [
    "if status == pywraplp.Solver.OPTIMAL:\n",
    "    print('Objective value =', solver.Objective().Value())\n",
    "    for j in range(m):\n",
    "        print(x[j].name(), ' = ', x[j].solution_value())\n",
    "    print()\n",
    "    print(f\"Problem solved in {solver.wall_time()} milliseconds\")\n",
    "    print(f\"Problem solved in {solver.iterations()} iterations\")\n",
    "    print(f\"Problem solved in {solver.nodes()} branch-and-bound nodes\")\n",
    "else:\n",
    "    print(\"The problem does not have an optimal solution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8221d57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': '127.0.0.1:31357',\n",
       " 'object_store_address': '/tmp/ray/session_2022-03-08_23-50-58_472144_7006/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-03-08_23-50-58_472144_7006/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-03-08_23-50-58_472144_7006',\n",
       " 'metrics_export_port': 63347,\n",
       " 'gcs_address': '127.0.0.1:56774',\n",
       " 'node_id': '4999ca31ed47f53247f8f6e2e18a26b41300238b0e7bac4de08f9491'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5029c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"framework\"] = \"torch\"\n",
    "config[\"env_config\"] = {}\n",
    "#config['kl_coeff'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99dd8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Discrete(ubound + 1)\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            'rem': gym.spaces.Box(low=np.zeros(n), high=b, dtype=np.float64), \n",
    "            'j': gym.spaces.Discrete(m + 1)})\n",
    "        self.state = {'rem': np.array(b), 'j': 0}\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = {'rem': np.array(b), 'j': 0}\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print('current state:', self.state)   \n",
    "        # print('action taken:', action)\n",
    "        j = self.state['j']\n",
    "        rem = self.state['rem'] - c[:,j] * action\n",
    "        if np.any(rem < 0):\n",
    "            self.reward = -1\n",
    "        else:\n",
    "            self.reward = action * p[j]\n",
    "            j += 1\n",
    "            self.state = {'rem': rem, 'j': j}\n",
    "            \n",
    "        # print('reward:', self.reward)\n",
    "        # print('next state:', self.state)\n",
    "        \n",
    "        if j == m: \n",
    "            self.done = True\n",
    "        else:\n",
    "            self.done = False\n",
    "            \n",
    "        return self.state, self.reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051efa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.env_checker import check_env\n",
    "# env = MyEnv(config)\n",
    "# check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37283666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1bf578e80>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = MyEnv(config)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79999803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  0\n",
      "obs:  {'rem': array([ 8., 20., 21.]), 'j': 1} reward:  0.0\n",
      "action:  1\n",
      "obs:  {'rem': array([ 7.4, 12.9, 15.7]), 'j': 2} reward:  3.5\n",
      "action:  0\n",
      "obs:  {'rem': array([ 7.4, 12.9, 15.7]), 'j': 3} reward:  0.0\n",
      "action:  1\n",
      "obs:  {'rem': array([7.1, 6.7, 9.7]), 'j': 4} reward:  2.6\n",
      "action:  1\n",
      "obs:  {'rem': array([1.4, 3.5, 0. ]), 'j': 5} reward:  4.5\n",
      "action:  0\n",
      "obs:  {'rem': array([1.4, 3.5, 0. ]), 'j': 6} reward:  0.0\n",
      "action:  0\n",
      "obs:  {'rem': array([1.4, 3.5, 0. ]), 'j': 7} reward:  0.0\n",
      "done. g =  10.6\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "g = 0\n",
    "for i in range(m):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    print('action: ', action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print('obs: ', obs, 'reward: ', reward)\n",
    "    g += reward\n",
    "    #env.render()\n",
    "    if done:\n",
    "      print('done. g = ', g)\n",
    "      obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d564763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 00:06:55,347\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7024)\u001b[0m 2022-03-09 00:06:55,303\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "agent = ppo.PPOTrainer(config=config, env=MyEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5150a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "mean episode length: 9.41745283018868\n",
      "max episode reward: 10.6\n",
      "mean episode reward: 3.394339622641509\n",
      "min episode reward: -8.7\n",
      "total episodes: 424\n",
      "\n",
      "i:  10\n",
      "mean episode length: 7.007005253940456\n",
      "max episode reward: 10.6\n",
      "mean episode reward: 10.592994746059544\n",
      "min episode reward: 9.6\n",
      "total episodes: 5939\n",
      "\n",
      "i:  20\n",
      "mean episode length: 47.71\n",
      "max episode reward: 8.6\n",
      "mean episode reward: -30.11\n",
      "min episode reward: -185.4\n",
      "total episodes: 9523\n",
      "\n",
      "i:  30\n",
      "mean episode length: 7.060070671378092\n",
      "max episode reward: 10.6\n",
      "mean episode reward: 10.512190812720847\n",
      "min episode reward: -3.3000000000000003\n",
      "total episodes: 13711\n",
      "\n",
      "i:  40\n",
      "mean episode length: 7.0\n",
      "max episode reward: 10.6\n",
      "mean episode reward: 10.58951048951049\n",
      "min episode reward: 7.6\n",
      "total episodes: 19374\n",
      "\n",
      "i:  50\n",
      "mean episode length: 7.0\n",
      "max episode reward: 10.6\n",
      "mean episode reward: 10.593169877408053\n",
      "min episode reward: 6.699999999999999\n",
      "total episodes: 24211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(51):\n",
    "   # Perform one iteration of training the policy with PPO\n",
    "   result = agent.train()\n",
    "   if i % 10 == 0:\n",
    "       #print(pretty_print(result))\n",
    "       print('i: ', i)\n",
    "       print('mean episode length:', result['episode_len_mean'])\n",
    "       print('max episode reward:', result['episode_reward_max'])\n",
    "       print('mean episode reward:', result['episode_reward_mean'])\n",
    "       print('min episode reward:', result['episode_reward_min'])\n",
    "       print('total episodes:', result['episodes_total'])\n",
    "       print()\n",
    "\n",
    "       checkpoint = agent.save()\n",
    "       #print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "685f28f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j = 0 action = 0 reward = 0\n",
      "j = 1 action = 1 reward = 0.0\n",
      "j = 2 action = 0 reward = 3.5\n",
      "j = 3 action = 1 reward = 0.0\n",
      "j = 4 action = 1 reward = 2.6\n",
      "j = 5 action = 0 reward = 4.5\n",
      "j = 6 action = 0 reward = 0.0\n",
      "10.6\n"
     ]
    }
   ],
   "source": [
    "env = MyEnv(config)\n",
    "state = env.reset()\n",
    "g = 0\n",
    "done = False\n",
    "reward = 0\n",
    "while not done:\n",
    "  action = agent.compute_action(state, explore = False)\n",
    "  print(f\"j = {state['j']} action = {action} reward = {reward}\")\n",
    "  state, reward, done, info = env.step(action)\n",
    "  g += reward\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "027b05b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved at /Users/vladimirsudakov/ray_results/PPOTrainer_MyEnv_2022-03-09_00-06-46dqgq572a/checkpoint_000051/checkpoint-51\n"
     ]
    }
   ],
   "source": [
    "print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b588f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
