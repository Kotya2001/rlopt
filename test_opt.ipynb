{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563ca54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ortools.linear_solver import pywraplp\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "import gym\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation import Episode, RolloutWorker\n",
    "from ray.rllib.utils.typing import AgentID, PolicyID\n",
    "from typing import Dict, Optional, TYPE_CHECKING\n",
    "from ray.rllib.policy import Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0299bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 7 # num vars\n",
    "n = 3  # num constraints\n",
    "ubound = 1 # upper bound of constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439e7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data simpling\n",
    "rand = np.random.RandomState(3)\n",
    "p = np.round(rand.random_sample(m)*5,1) # goal koef\n",
    "c = np.round(rand.random_sample((n,m))*10 * (rand.random_sample(m)*(p/5)*0.3 + 1),1) # constrants\n",
    "b = np.round(c.sum(axis = 1) * (rand.random_sample(n) * 0.5 + 0.3),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9e9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = pywraplp.Solver.CreateSolver('SCIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0331ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "for j in range(m):\n",
    "    x[j] = solver.IntVar(0, ubound, f\"x[{j}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450f4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    constraint_expr = [c[i,j] * x[j] for j in range(m)]\n",
    "    solver.Add(sum(constraint_expr) <= b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1555d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_expr = [p[j] * x[j] for j in range(m)]\n",
    "solver.Maximize(solver.Sum(obj_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac54271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver.EnableOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080bdc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = solver.Solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83ed1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value = 10.600000000000001\n",
      "x[0]  =  0.0\n",
      "x[1]  =  1.0\n",
      "x[2]  =  0.0\n",
      "x[3]  =  1.0\n",
      "x[4]  =  1.0\n",
      "x[5]  =  0.0\n",
      "x[6]  =  0.0\n",
      "\n",
      "Problem solved in 77 milliseconds\n",
      "Problem solved in 7 iterations\n",
      "Problem solved in 1 branch-and-bound nodes\n"
     ]
    }
   ],
   "source": [
    "if status == pywraplp.Solver.OPTIMAL:\n",
    "    print('Objective value =', solver.Objective().Value())\n",
    "    for j in range(m):\n",
    "        print(x[j].name(), ' = ', x[j].solution_value())\n",
    "    print()\n",
    "    print(f\"Problem solved in {solver.wall_time()} milliseconds\")\n",
    "    print(f\"Problem solved in {solver.iterations()} iterations\")\n",
    "    print(f\"Problem solved in {solver.nodes()} branch-and-bound nodes\")\n",
    "else:\n",
    "    print(\"The problem does not have an optimal solution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99dd8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Discrete(ubound + 1)\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            'rem': gym.spaces.Box(low=np.zeros(n), high=b, dtype=np.float64), \n",
    "            'j': gym.spaces.Discrete(m) #, \n",
    "            #'x': gym.spaces.Tuple([gym.spaces.Discrete(ubound + 1)]*m)\n",
    "        })\n",
    "        self.state = {'rem': np.array(b), 'j': 0} # , 'x': [0]*m\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = {'rem': np.array(b), 'j': 0} #, 'x': [0]*m\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print('current state:', self.state)   \n",
    "        # print('action taken:', action)\n",
    "        j = self.state['j']\n",
    "        rem = self.state['rem'] - c[:,j] * action\n",
    "        if np.any(rem < 0):\n",
    "            self.reward = -1\n",
    "        else:\n",
    "            self.reward = action * p[j]\n",
    "            # self.state['x'][j] = action \n",
    "            self.state['rem'] = rem\n",
    "            j += 1\n",
    "            if j == m: \n",
    "                self.done = True\n",
    "            else:\n",
    "                self.state['j'] = j\n",
    "                self.done = False\n",
    "        # print('reward:', self.reward)\n",
    "        # print('next state:', self.state)    \n",
    "        return self.state, self.reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8221d57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': '127.0.0.1:45322',\n",
       " 'object_store_address': '/tmp/ray/session_2022-04-02_17-54-38_713903_7998/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-04-02_17-54-38_713903_7998/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-04-02_17-54-38_713903_7998',\n",
       " 'metrics_export_port': 65364,\n",
       " 'gcs_address': '127.0.0.1:63152',\n",
       " 'node_id': '8358b9ab4d5adf88d9a9fc3d37597897ca71818d5795e41c980a6417'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c5eb6e-b2a1-4bf5-a9ca-28b824202f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleCallback(DefaultCallbacks):\n",
    "    \n",
    "    def __init__(self, legacy_callbacks_dict: Dict[str, callable] = None):\n",
    "        self.best_reward = -666666666\n",
    "        self.best_actions = []\n",
    "        self.legacy_callbacks = legacy_callbacks_dict or {}\n",
    "        \n",
    "    def on_postprocess_trajectory(\n",
    "            self, *, worker: \"RolloutWorker\", episode: Episode,\n",
    "            agent_id: AgentID, policy_id: PolicyID,\n",
    "            policies: Dict[PolicyID, Policy], postprocessed_batch: SampleBatch,\n",
    "            original_batches: Dict[AgentID, SampleBatch], **kwargs) -> None:\n",
    "        \n",
    "        sample_obj = original_batches[agent_id][1]\n",
    "        rewards = sample_obj.columns(['rewards'])[0]\n",
    "        total_reward = np.sum(rewards)\n",
    "        actions = sample_obj.columns(['actions'])[0]\n",
    "        \n",
    "        if total_reward > self.best_reward and np.all(rewards >= 0):\n",
    "            self.best_reward = total_reward\n",
    "            self.best_actions = actions\n",
    "            episode.hist_data[\"best_reward\"] = [total_reward]\n",
    "            episode.hist_data[\"best_actions\"] = [actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5029c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"framework\"] = \"torch\"\n",
    "config[\"env_config\"] = {}\n",
    "#config['kl_coeff'] = 0.0\n",
    "config[\"callbacks\"] = SampleCallback\n",
    "config[\"log_level\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "051efa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = MyEnv(config)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37283666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x130a4fe50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = MyEnv(config)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79999803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  1\n",
      "obs:  {'rem': array([ 5.8, 12.8, 13.6]), 'j': 1} reward:  2.8\n",
      "action:  1\n",
      "obs:  {'rem': array([5.2, 5.7, 8.3]), 'j': 2} reward:  3.5\n",
      "action:  0\n",
      "obs:  {'rem': array([5.2, 5.7, 8.3]), 'j': 3} reward:  0.0\n",
      "action:  0\n",
      "obs:  {'rem': array([5.2, 5.7, 8.3]), 'j': 4} reward:  0.0\n",
      "action:  0\n",
      "obs:  {'rem': array([5.2, 5.7, 8.3]), 'j': 5} reward:  0.0\n",
      "action:  0\n",
      "obs:  {'rem': array([5.2, 5.7, 8.3]), 'j': 6} reward:  0.0\n",
      "action:  0\n",
      "obs:  {'rem': array([5.2, 5.7, 8.3]), 'j': 6} reward:  0.0\n",
      "done. g =  6.3\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "g = 0\n",
    "for i in range(m):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    print('action: ', action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print('obs: ', obs, 'reward: ', reward)\n",
    "    g += reward\n",
    "    #env.render()\n",
    "    if done:\n",
    "      print('done. g = ', g)\n",
    "      obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d564763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-02 18:01:15,471\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent = ppo.PPOTrainer(config=config, env=MyEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5150a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "mean episode length: 9.516666666666667\n",
      "max episode reward: 10.6\n",
      "mean episode reward: 3.3371428571428567\n",
      "min episode reward: -6.7\n",
      "total episodes: 420\n",
      "solution: 10.6 [0 1 0 1 1 0 0]\n",
      "i:  10\n",
      "mean episode length: 7.019298245614035\n",
      "max episode reward: 10.6\n",
      "mean episode reward: 10.305438596491227\n",
      "min episode reward: 3.2\n",
      "total episodes: 5902\n",
      "solution: 10.6 [0 1 0 1 1 0 0]\n",
      "i:  20\n",
      "mean episode length: 7.0\n",
      "max episode reward: 10.6\n",
      "mean episode reward: 10.589842381786339\n",
      "min episode reward: 7.6\n",
      "total episodes: 11614\n",
      "solution: 10.6 [0 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "best_g = 0\n",
    "best_actions = []\n",
    "for i in range(21):\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = agent.train()\n",
    "    if 'best_reward' in result['hist_stats'] and len(result['hist_stats']['best_reward']) > 0 and \\\n",
    "        ( best_g < result['hist_stats']['best_reward'][-1] or best_actions == []):\n",
    "        best_g = result['hist_stats']['best_reward'][-1]\n",
    "        best_actions = result['hist_stats']['best_actions'][-1]\n",
    "    if i % 10 == 0:\n",
    "        #print(pretty_print(result))\n",
    "        print('i: ', i)\n",
    "        print('mean episode length:', result['episode_len_mean'])\n",
    "        print('max episode reward:', result['episode_reward_max'])\n",
    "        print('mean episode reward:', result['episode_reward_mean'])\n",
    "        print('min episode reward:', result['episode_reward_min'])\n",
    "        print('total episodes:', result['episodes_total'])\n",
    "        print('solution:', best_g, best_actions)\n",
    "        checkpoint = agent.save()\n",
    "        #print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "685f28f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' no need agent\\nenv = MyEnv(config)\\nstate = env.reset()\\ng = 0\\ndone = False\\nreward = 0\\nwhile not done:\\n  action = agent.compute_action(state, explore = False)\\n  print(f\"j = {state[\\'j\\']} action = {action} reward = {reward}\")\\n  state, reward, done, info = env.step(action)\\n  g += reward\\nprint(g) '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" no need agent\n",
    "env = MyEnv(config)\n",
    "state = env.reset()\n",
    "g = 0\n",
    "done = False\n",
    "reward = 0\n",
    "while not done:\n",
    "  action = agent.compute_action(state, explore = False)\n",
    "  print(f\"j = {state['j']} action = {action} reward = {reward}\")\n",
    "  state, reward, done, info = env.step(action)\n",
    "  g += reward\n",
    "print(g) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0b588f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j = 0 action = 0 reward = 0.0\n",
      "j = 1 action = 1 reward = 3.5\n",
      "j = 2 action = 0 reward = 0.0\n",
      "j = 3 action = 1 reward = 2.6\n",
      "j = 4 action = 1 reward = 4.5\n",
      "j = 5 action = 0 reward = 0.0\n",
      "j = 6 action = 0 reward = 0.0\n",
      "10.6\n"
     ]
    }
   ],
   "source": [
    "for j, x in enumerate(best_actions):\n",
    "    print(f\"j = {j} action = {x} reward = {x*p[j]}\")\n",
    "print(best_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8294e6-80b5-4593-a606-ddec32d81f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
